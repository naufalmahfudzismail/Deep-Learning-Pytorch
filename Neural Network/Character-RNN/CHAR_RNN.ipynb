{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Character-Level LSTM in PyTorch\n",
    "#In this notebook, I'll construct a character-level LSTM with PyTorch. The network will train character by character on some text, then generate new text character by character. As an example, I will train on Anna Karenina. This model will be able to generate new text based on the text from the book!\n",
    "#This network is based off of Andrej Karpathy's post on RNNs and implementation in Torch. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open text file dan baca data sebagai text\n",
    "with open('data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check out the first 100 characters, make sure everything is peachy. According to the American Book Review, this is the 6th best first line of a book ever.\n",
    "\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode text dan petakan masing masing karakter menjadi interger dan vise verca\n",
    "\n",
    "#kita membuat 2 kamus\n",
    "#1. int2char, dimana interger dipetakan menjadi character\n",
    "#2. char2int, dimana character dipetakan menjadi integer yg unik\n",
    "\n",
    "chars = tuple(set(text))\n",
    "intToChar = dict(enumerate(chars))\n",
    "charToInt = {ch : ii for ii, ch in intToChar.items()}\n",
    "\n",
    "#encode the text\n",
    "encoded = np.array([charToInt[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 69, 14, 78, 39, 55, 44, 17, 13, 53, 53, 53, 11, 14, 78, 78, 21,\n",
       "       17, 67, 14, 68, 36, 22, 36, 55,  9, 17, 14, 44, 55, 17, 14, 22, 22,\n",
       "       17, 14, 22, 36, 20, 55, 80, 17, 55, 70, 55, 44, 21, 17, 45, 32, 69,\n",
       "       14, 78, 78, 21, 17, 67, 14, 68, 36, 22, 21, 17, 36,  9, 17, 45, 32,\n",
       "       69, 14, 78, 78, 21, 17, 36, 32, 17, 36, 39,  9, 17, 31, 66, 32, 53,\n",
       "       66, 14, 21, 57, 53, 53, 40, 70, 55, 44, 21, 39, 69, 36, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing the data\n",
    "#As you can see in our char-RNN image above, our LSTM expects an input that is one-hot encoded meaning that each character is converted into an integer (via our created dictionary) and then converted into a column vector where only it's corresponding integer index will have the value of 1 and the rest of the vector will be filled with 0's. Since we're one-hot encoding the data, let's make a function to do that!\n",
    "\n",
    "def oneHotEncode(arr, nLabels):\n",
    "    \n",
    "    #inisialisasi encoded array\n",
    "    oneHot = np.zeros((np.multiply(*arr.shape), nLabels), dtype = np.float32)\n",
    "    \n",
    "    #isi element dengan ones\n",
    "    oneHot[np.arange(oneHot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    #terakhir, bentuk ke array yg asli\n",
    "    oneHot = oneHot.reshape((*arr.shape, nLabels))\n",
    "    \n",
    "    return oneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "#Check fungsi berkerja dengan semestinya\n",
    "testSeq = np.array([[3, 5, 1]])\n",
    "oneHot  = oneHotEncode(testSeq, 8)\n",
    "\n",
    "print(oneHot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatches(arr, batchSize, seqLength):\n",
    "    ''' Membuat generator yang mengembalikan ukuran batch\n",
    "     argument :\n",
    "     arr : Array yang ingin di buat per batch\n",
    "     batchSize : jumlah ukuran batch\n",
    "     seqLenght : panjang encode char\n",
    "     '''\n",
    "    \n",
    "    batchSizeTotal = batchSize * seqLength\n",
    "    #total jumat batch yang dapat di buat\n",
    "    nBatches = len(arr)//batchSizeTotal\n",
    "    \n",
    "    #menyimpan karakter yang hanya dapat membuat batch nya penuh\n",
    "    arr = arr[:nBatches * batchSizeTotal]\n",
    "    #bentuk ulang ke bari sbatchSize\n",
    "    arr = arr.reshape((batchSize, -1))\n",
    "    \n",
    "    #iterasi lewat array, 1 sequen per waktu\n",
    "    \n",
    "    for n in range(0, arr.shape[1], seqLength):\n",
    "        # feature\n",
    "        x = arr[:, n : n + seqLength]\n",
    "        #target bergeser 1\n",
    "        y = np.zeros_like(x)\n",
    "        try :\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n + seqLength]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test implementasi \n",
    "\n",
    "batches = getBatches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[19 69 14 78 39 55 44 17 13 53]\n",
      " [ 9 31 32 17 39 69 14 39 17 14]\n",
      " [55 32 42 17 31 44 17 14 17 67]\n",
      " [ 9 17 39 69 55 17 82 69 36 55]\n",
      " [17  9 14 66 17 69 55 44 17 39]\n",
      " [82 45  9  9 36 31 32 17 14 32]\n",
      " [17 26 32 32 14 17 69 14 42 17]\n",
      " [71 41 22 31 32  9 20 21 57 17]]\n",
      "\n",
      "y\n",
      " [[69 14 78 39 55 44 17 13 53 53]\n",
      " [31 32 17 39 69 14 39 17 14 39]\n",
      " [32 42 17 31 44 17 14 17 67 31]\n",
      " [17 39 69 55 17 82 69 36 55 67]\n",
      " [ 9 14 66 17 69 55 44 17 39 55]\n",
      " [45  9  9 36 31 32 17 14 32 42]\n",
      " [26 32 32 14 17 69 14 42 17  9]\n",
      " [41 22 31 32  9 20 21 57 17 51]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "source": [
    "#Cek GPU tersedia\n",
    "trainOnGPU = torch.cuda.is_available()\n",
    "if(trainOnGPU):\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, nHidden = 256, nLayers = 2,\n",
    "                dropProb = 0.5, lr = 0.001):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dropProb = dropProb\n",
    "        self.nLayers = nLayers\n",
    "        self.nHidden = nHidden\n",
    "        self.lr  = lr\n",
    "        \n",
    "        #Membuat kamus character\n",
    "        self.chars = tokens\n",
    "        self.intToChar = dict(enumerate(self.chars))\n",
    "        self.charToInt = {\n",
    "            ch : ii for ii, ch in self.intToChar.items() }\n",
    "        \n",
    "        #define the LTSM\n",
    "        self.lstm = nn.LSTM(len(self.chars), nHidden, nLayers,\n",
    "                           dropout = dropProb, batch_first = True)\n",
    "        \n",
    "        #define a dropout Layer\n",
    "        self.dropout = nn.Dropout(dropProb)\n",
    "        \n",
    "        #define akhir, koneksi output layer\n",
    "        self.fc = nn.Linear(nHidden, len(self.chars))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        \n",
    "        ## ambil output dan hidden yang baru dari LSTM\n",
    "        rOutput, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        #Lewati dropout layer\n",
    "        out = self.dropout(rOutput)\n",
    "        \n",
    "        #Stack output LSTM dengan view\n",
    "        #gunakan contiguous untuk membentuk output\n",
    "        out = out.contiguous().view(-1, self.nHidden)\n",
    "        \n",
    "        #taro x dengan melewati layer yang sudah terkoneksi\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        #return hasil akhir dan hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    def initHidden(self, batchSize):\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if(trainOnGPU):\n",
    "             hidden = (weight.new(self.nLayers, batchSize, self.nHidden).zero_().cuda(),\n",
    "                  weight.new(self.nLayers, batchSize, self.nHidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.nLayers, batchSize, self.nHidden).zero_(),\n",
    "                      weight.new(self.nLayers, batchSize, self.nHidden).zero_())\n",
    "            \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs = 10, batchSize = 10, seqLength = 50, lr = 0.001, clip = 5, valFrac = 0.1, printEvery = 10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Membuat data training dan validasi data\n",
    "    valIdx = int(len(data)*(1-valFrac))\n",
    "    data, valData = data[:valIdx], data[valIdx:]\n",
    "    \n",
    "    if(trainOnGPU):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    nChars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # inisialisai hidden state\n",
    "        h = net.initHidden(batchSize)\n",
    "        \n",
    "        for x, y in getBatches(data, batchSize, seqLength):\n",
    "            counter += 1\n",
    "            \n",
    "            # Encode data dan buat mereka menjadi torch Tensor\n",
    "            x = oneHotEncode(x, nChars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(trainOnGPU):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Buat variabel baru untuk hidden state, bagaimanapun\n",
    "            # kita backprop semua training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # akumulasi gradien = 0\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get output model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # kalkulasi loss dan lakukan backprop\n",
    "            loss = criterion(output, targets.view(batchSize * seqLength).type(torch.cuda.LongTensor))\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` membantu mencegah ledakan gradien di RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % printEvery == 0:\n",
    "                # Get validation loss\n",
    "                valH = net.initHidden(batchSize)\n",
    "                valLosses = []\n",
    "                net.eval()\n",
    "                for x, y in getBatches(valData, batchSize, seqLength):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = oneHotEncode(x, nChars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    valH = tuple([each.data for each in valH])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(trainOnGPU):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, valH = net(inputs, valH)\n",
    "                    valLoss = criterion(output, targets.view(batchSize * seqLength).type(torch.cuda.LongTensor))\n",
    "                \n",
    "                    valLosses.append(valLoss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e + 1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(valLosses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Inisialisasi Model\n",
    "# define and print the net\n",
    "nHidden = 512\n",
    "nLayers = 2\n",
    "\n",
    "net = CharRNN(chars, nHidden, nLayers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 3.2472... Val Loss: 3.1858\n",
      "Epoch: 1/20... Step: 20... Loss: 3.1374... Val Loss: 3.1285\n",
      "Epoch: 1/20... Step: 30... Loss: 3.1380... Val Loss: 3.1207\n",
      "Epoch: 1/20... Step: 40... Loss: 3.1092... Val Loss: 3.1190\n",
      "Epoch: 1/20... Step: 50... Loss: 3.1385... Val Loss: 3.1161\n",
      "Epoch: 1/20... Step: 60... Loss: 3.1151... Val Loss: 3.1130\n",
      "Epoch: 1/20... Step: 70... Loss: 3.1016... Val Loss: 3.1088\n",
      "Epoch: 1/20... Step: 80... Loss: 3.1116... Val Loss: 3.0961\n",
      "Epoch: 1/20... Step: 90... Loss: 3.0859... Val Loss: 3.0665\n",
      "Epoch: 1/20... Step: 100... Loss: 3.0185... Val Loss: 2.9988\n",
      "Epoch: 1/20... Step: 110... Loss: 2.9259... Val Loss: 2.8949\n",
      "Epoch: 1/20... Step: 120... Loss: 2.7962... Val Loss: 2.7973\n",
      "Epoch: 1/20... Step: 130... Loss: 2.7037... Val Loss: 2.6951\n",
      "Epoch: 2/20... Step: 140... Loss: 2.6380... Val Loss: 2.5797\n",
      "Epoch: 2/20... Step: 150... Loss: 2.5602... Val Loss: 2.5148\n",
      "Epoch: 2/20... Step: 160... Loss: 2.5093... Val Loss: 2.4685\n",
      "Epoch: 2/20... Step: 170... Loss: 2.4372... Val Loss: 2.4249\n",
      "Epoch: 2/20... Step: 180... Loss: 2.4142... Val Loss: 2.3950\n",
      "Epoch: 2/20... Step: 190... Loss: 2.3655... Val Loss: 2.3616\n",
      "Epoch: 2/20... Step: 200... Loss: 2.3606... Val Loss: 2.3418\n",
      "Epoch: 2/20... Step: 210... Loss: 2.3383... Val Loss: 2.3094\n",
      "Epoch: 2/20... Step: 220... Loss: 2.2905... Val Loss: 2.2725\n",
      "Epoch: 2/20... Step: 230... Loss: 2.2748... Val Loss: 2.2429\n",
      "Epoch: 2/20... Step: 240... Loss: 2.2451... Val Loss: 2.2181\n",
      "Epoch: 2/20... Step: 250... Loss: 2.1874... Val Loss: 2.1901\n",
      "Epoch: 2/20... Step: 260... Loss: 2.1574... Val Loss: 2.1616\n",
      "Epoch: 2/20... Step: 270... Loss: 2.1623... Val Loss: 2.1415\n",
      "Epoch: 3/20... Step: 280... Loss: 2.1548... Val Loss: 2.1170\n",
      "Epoch: 3/20... Step: 290... Loss: 2.1318... Val Loss: 2.0975\n",
      "Epoch: 3/20... Step: 300... Loss: 2.1038... Val Loss: 2.0731\n",
      "Epoch: 3/20... Step: 310... Loss: 2.0651... Val Loss: 2.0553\n",
      "Epoch: 3/20... Step: 320... Loss: 2.0379... Val Loss: 2.0322\n",
      "Epoch: 3/20... Step: 330... Loss: 2.0095... Val Loss: 2.0141\n",
      "Epoch: 3/20... Step: 340... Loss: 2.0364... Val Loss: 1.9970\n",
      "Epoch: 3/20... Step: 350... Loss: 2.0188... Val Loss: 1.9819\n",
      "Epoch: 3/20... Step: 360... Loss: 1.9495... Val Loss: 1.9596\n",
      "Epoch: 3/20... Step: 370... Loss: 1.9731... Val Loss: 1.9434\n",
      "Epoch: 3/20... Step: 380... Loss: 1.9515... Val Loss: 1.9274\n",
      "Epoch: 3/20... Step: 390... Loss: 1.9194... Val Loss: 1.9130\n",
      "Epoch: 3/20... Step: 400... Loss: 1.8896... Val Loss: 1.8970\n",
      "Epoch: 3/20... Step: 410... Loss: 1.9023... Val Loss: 1.8849\n",
      "Epoch: 4/20... Step: 420... Loss: 1.9029... Val Loss: 1.8720\n",
      "Epoch: 4/20... Step: 430... Loss: 1.8879... Val Loss: 1.8555\n",
      "Epoch: 4/20... Step: 440... Loss: 1.8655... Val Loss: 1.8474\n",
      "Epoch: 4/20... Step: 450... Loss: 1.8137... Val Loss: 1.8271\n",
      "Epoch: 4/20... Step: 460... Loss: 1.7898... Val Loss: 1.8194\n",
      "Epoch: 4/20... Step: 470... Loss: 1.8378... Val Loss: 1.8095\n",
      "Epoch: 4/20... Step: 480... Loss: 1.8136... Val Loss: 1.7966\n",
      "Epoch: 4/20... Step: 490... Loss: 1.8130... Val Loss: 1.7847\n",
      "Epoch: 4/20... Step: 500... Loss: 1.8095... Val Loss: 1.7754\n",
      "Epoch: 4/20... Step: 510... Loss: 1.7822... Val Loss: 1.7651\n",
      "Epoch: 4/20... Step: 520... Loss: 1.8020... Val Loss: 1.7582\n",
      "Epoch: 4/20... Step: 530... Loss: 1.7590... Val Loss: 1.7468\n",
      "Epoch: 4/20... Step: 540... Loss: 1.7225... Val Loss: 1.7385\n",
      "Epoch: 4/20... Step: 550... Loss: 1.7647... Val Loss: 1.7262\n",
      "Epoch: 5/20... Step: 560... Loss: 1.7241... Val Loss: 1.7223\n",
      "Epoch: 5/20... Step: 570... Loss: 1.7231... Val Loss: 1.7092\n",
      "Epoch: 5/20... Step: 580... Loss: 1.6969... Val Loss: 1.6955\n",
      "Epoch: 5/20... Step: 590... Loss: 1.7001... Val Loss: 1.6875\n",
      "Epoch: 5/20... Step: 600... Loss: 1.6902... Val Loss: 1.6804\n",
      "Epoch: 5/20... Step: 610... Loss: 1.6792... Val Loss: 1.6755\n",
      "Epoch: 5/20... Step: 620... Loss: 1.6791... Val Loss: 1.6692\n",
      "Epoch: 5/20... Step: 630... Loss: 1.6957... Val Loss: 1.6634\n",
      "Epoch: 5/20... Step: 640... Loss: 1.6659... Val Loss: 1.6559\n",
      "Epoch: 5/20... Step: 650... Loss: 1.6538... Val Loss: 1.6485\n",
      "Epoch: 5/20... Step: 660... Loss: 1.6342... Val Loss: 1.6470\n",
      "Epoch: 5/20... Step: 670... Loss: 1.6549... Val Loss: 1.6349\n",
      "Epoch: 5/20... Step: 680... Loss: 1.6558... Val Loss: 1.6279\n",
      "Epoch: 5/20... Step: 690... Loss: 1.6258... Val Loss: 1.6253\n",
      "Epoch: 6/20... Step: 700... Loss: 1.6308... Val Loss: 1.6198\n",
      "Epoch: 6/20... Step: 710... Loss: 1.6125... Val Loss: 1.6101\n",
      "Epoch: 6/20... Step: 720... Loss: 1.5992... Val Loss: 1.5986\n",
      "Epoch: 6/20... Step: 730... Loss: 1.6203... Val Loss: 1.5993\n",
      "Epoch: 6/20... Step: 740... Loss: 1.5855... Val Loss: 1.5948\n",
      "Epoch: 6/20... Step: 750... Loss: 1.5700... Val Loss: 1.5903\n",
      "Epoch: 6/20... Step: 760... Loss: 1.6034... Val Loss: 1.5830\n",
      "Epoch: 6/20... Step: 770... Loss: 1.5934... Val Loss: 1.5821\n",
      "Epoch: 6/20... Step: 780... Loss: 1.5647... Val Loss: 1.5790\n",
      "Epoch: 6/20... Step: 790... Loss: 1.5574... Val Loss: 1.5682\n",
      "Epoch: 6/20... Step: 800... Loss: 1.5824... Val Loss: 1.5672\n",
      "Epoch: 6/20... Step: 810... Loss: 1.5697... Val Loss: 1.5626\n",
      "Epoch: 6/20... Step: 820... Loss: 1.5322... Val Loss: 1.5591\n",
      "Epoch: 6/20... Step: 830... Loss: 1.5799... Val Loss: 1.5548\n",
      "Epoch: 7/20... Step: 840... Loss: 1.5391... Val Loss: 1.5500\n",
      "Epoch: 7/20... Step: 850... Loss: 1.5518... Val Loss: 1.5441\n",
      "Epoch: 7/20... Step: 860... Loss: 1.5313... Val Loss: 1.5380\n",
      "Epoch: 7/20... Step: 870... Loss: 1.5435... Val Loss: 1.5346\n",
      "Epoch: 7/20... Step: 880... Loss: 1.5365... Val Loss: 1.5321\n",
      "Epoch: 7/20... Step: 890... Loss: 1.5358... Val Loss: 1.5286\n",
      "Epoch: 7/20... Step: 900... Loss: 1.5232... Val Loss: 1.5254\n",
      "Epoch: 7/20... Step: 910... Loss: 1.4939... Val Loss: 1.5219\n",
      "Epoch: 7/20... Step: 920... Loss: 1.5187... Val Loss: 1.5193\n",
      "Epoch: 7/20... Step: 930... Loss: 1.5029... Val Loss: 1.5131\n",
      "Epoch: 7/20... Step: 940... Loss: 1.5015... Val Loss: 1.5109\n",
      "Epoch: 7/20... Step: 950... Loss: 1.5197... Val Loss: 1.5062\n",
      "Epoch: 7/20... Step: 960... Loss: 1.5245... Val Loss: 1.5029\n",
      "Epoch: 7/20... Step: 970... Loss: 1.5268... Val Loss: 1.4999\n",
      "Epoch: 8/20... Step: 980... Loss: 1.4862... Val Loss: 1.4951\n",
      "Epoch: 8/20... Step: 990... Loss: 1.4916... Val Loss: 1.4914\n",
      "Epoch: 8/20... Step: 1000... Loss: 1.4913... Val Loss: 1.4897\n",
      "Epoch: 8/20... Step: 1010... Loss: 1.5161... Val Loss: 1.4887\n",
      "Epoch: 8/20... Step: 1020... Loss: 1.4939... Val Loss: 1.4841\n",
      "Epoch: 8/20... Step: 1030... Loss: 1.4780... Val Loss: 1.4839\n",
      "Epoch: 8/20... Step: 1040... Loss: 1.4916... Val Loss: 1.4837\n",
      "Epoch: 8/20... Step: 1050... Loss: 1.4739... Val Loss: 1.4786\n",
      "Epoch: 8/20... Step: 1060... Loss: 1.4716... Val Loss: 1.4769\n",
      "Epoch: 8/20... Step: 1070... Loss: 1.4675... Val Loss: 1.4696\n",
      "Epoch: 8/20... Step: 1080... Loss: 1.4770... Val Loss: 1.4684\n",
      "Epoch: 8/20... Step: 1090... Loss: 1.4562... Val Loss: 1.4632\n",
      "Epoch: 8/20... Step: 1100... Loss: 1.4484... Val Loss: 1.4654\n",
      "Epoch: 8/20... Step: 1110... Loss: 1.4590... Val Loss: 1.4629\n",
      "Epoch: 9/20... Step: 1120... Loss: 1.4734... Val Loss: 1.4612\n",
      "Epoch: 9/20... Step: 1130... Loss: 1.4711... Val Loss: 1.4557\n",
      "Epoch: 9/20... Step: 1140... Loss: 1.4644... Val Loss: 1.4536\n",
      "Epoch: 9/20... Step: 1150... Loss: 1.4785... Val Loss: 1.4532\n",
      "Epoch: 9/20... Step: 1160... Loss: 1.4333... Val Loss: 1.4513\n",
      "Epoch: 9/20... Step: 1170... Loss: 1.4338... Val Loss: 1.4479\n",
      "Epoch: 9/20... Step: 1180... Loss: 1.4338... Val Loss: 1.4471\n",
      "Epoch: 9/20... Step: 1190... Loss: 1.4572... Val Loss: 1.4437\n",
      "Epoch: 9/20... Step: 1200... Loss: 1.4116... Val Loss: 1.4410\n",
      "Epoch: 9/20... Step: 1210... Loss: 1.4250... Val Loss: 1.4374\n",
      "Epoch: 9/20... Step: 1220... Loss: 1.4303... Val Loss: 1.4365\n",
      "Epoch: 9/20... Step: 1230... Loss: 1.4068... Val Loss: 1.4369\n",
      "Epoch: 9/20... Step: 1240... Loss: 1.4093... Val Loss: 1.4309\n",
      "Epoch: 9/20... Step: 1250... Loss: 1.4250... Val Loss: 1.4306\n",
      "Epoch: 10/20... Step: 1260... Loss: 1.4253... Val Loss: 1.4276\n",
      "Epoch: 10/20... Step: 1270... Loss: 1.4234... Val Loss: 1.4282\n",
      "Epoch: 10/20... Step: 1280... Loss: 1.4345... Val Loss: 1.4241\n",
      "Epoch: 10/20... Step: 1290... Loss: 1.4209... Val Loss: 1.4239\n",
      "Epoch: 10/20... Step: 1300... Loss: 1.4058... Val Loss: 1.4230\n",
      "Epoch: 10/20... Step: 1310... Loss: 1.4228... Val Loss: 1.4150\n",
      "Epoch: 10/20... Step: 1320... Loss: 1.3867... Val Loss: 1.4178\n",
      "Epoch: 10/20... Step: 1330... Loss: 1.3916... Val Loss: 1.4140\n",
      "Epoch: 10/20... Step: 1340... Loss: 1.3878... Val Loss: 1.4130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20... Step: 1350... Loss: 1.3714... Val Loss: 1.4100\n",
      "Epoch: 10/20... Step: 1360... Loss: 1.3762... Val Loss: 1.4114\n",
      "Epoch: 10/20... Step: 1370... Loss: 1.3716... Val Loss: 1.4054\n",
      "Epoch: 10/20... Step: 1380... Loss: 1.4042... Val Loss: 1.3994\n",
      "Epoch: 10/20... Step: 1390... Loss: 1.4129... Val Loss: 1.4068\n",
      "Epoch: 11/20... Step: 1400... Loss: 1.4201... Val Loss: 1.4022\n",
      "Epoch: 11/20... Step: 1410... Loss: 1.4288... Val Loss: 1.4030\n",
      "Epoch: 11/20... Step: 1420... Loss: 1.4162... Val Loss: 1.3964\n",
      "Epoch: 11/20... Step: 1430... Loss: 1.3875... Val Loss: 1.4018\n",
      "Epoch: 11/20... Step: 1440... Loss: 1.4089... Val Loss: 1.3974\n",
      "Epoch: 11/20... Step: 1450... Loss: 1.3348... Val Loss: 1.3991\n",
      "Epoch: 11/20... Step: 1460... Loss: 1.3598... Val Loss: 1.3938\n",
      "Epoch: 11/20... Step: 1470... Loss: 1.3539... Val Loss: 1.3917\n",
      "Epoch: 11/20... Step: 1480... Loss: 1.3715... Val Loss: 1.3885\n",
      "Epoch: 11/20... Step: 1490... Loss: 1.3652... Val Loss: 1.3893\n",
      "Epoch: 11/20... Step: 1500... Loss: 1.3543... Val Loss: 1.3929\n",
      "Epoch: 11/20... Step: 1510... Loss: 1.3366... Val Loss: 1.3885\n",
      "Epoch: 11/20... Step: 1520... Loss: 1.3746... Val Loss: 1.3841\n",
      "Epoch: 12/20... Step: 1530... Loss: 1.4221... Val Loss: 1.3861\n",
      "Epoch: 12/20... Step: 1540... Loss: 1.3774... Val Loss: 1.3816\n",
      "Epoch: 12/20... Step: 1550... Loss: 1.3859... Val Loss: 1.3811\n",
      "Epoch: 12/20... Step: 1560... Loss: 1.3945... Val Loss: 1.3764\n",
      "Epoch: 12/20... Step: 1570... Loss: 1.3473... Val Loss: 1.3783\n",
      "Epoch: 12/20... Step: 1580... Loss: 1.3246... Val Loss: 1.3791\n",
      "Epoch: 12/20... Step: 1590... Loss: 1.3159... Val Loss: 1.3754\n",
      "Epoch: 12/20... Step: 1600... Loss: 1.3482... Val Loss: 1.3765\n",
      "Epoch: 12/20... Step: 1610... Loss: 1.3354... Val Loss: 1.3718\n",
      "Epoch: 12/20... Step: 1620... Loss: 1.3289... Val Loss: 1.3685\n",
      "Epoch: 12/20... Step: 1630... Loss: 1.3590... Val Loss: 1.3659\n",
      "Epoch: 12/20... Step: 1640... Loss: 1.3274... Val Loss: 1.3679\n",
      "Epoch: 12/20... Step: 1650... Loss: 1.3096... Val Loss: 1.3643\n",
      "Epoch: 12/20... Step: 1660... Loss: 1.3598... Val Loss: 1.3611\n",
      "Epoch: 13/20... Step: 1670... Loss: 1.3328... Val Loss: 1.3622\n",
      "Epoch: 13/20... Step: 1680... Loss: 1.3496... Val Loss: 1.3575\n",
      "Epoch: 13/20... Step: 1690... Loss: 1.3169... Val Loss: 1.3583\n",
      "Epoch: 13/20... Step: 1700... Loss: 1.3227... Val Loss: 1.3523\n",
      "Epoch: 13/20... Step: 1710... Loss: 1.3038... Val Loss: 1.3547\n",
      "Epoch: 13/20... Step: 1720... Loss: 1.3107... Val Loss: 1.3583\n",
      "Epoch: 13/20... Step: 1730... Loss: 1.3525... Val Loss: 1.3510\n",
      "Epoch: 13/20... Step: 1740... Loss: 1.3117... Val Loss: 1.3550\n",
      "Epoch: 13/20... Step: 1750... Loss: 1.2948... Val Loss: 1.3517\n",
      "Epoch: 13/20... Step: 1760... Loss: 1.3081... Val Loss: 1.3447\n",
      "Epoch: 13/20... Step: 1770... Loss: 1.3279... Val Loss: 1.3478\n",
      "Epoch: 13/20... Step: 1780... Loss: 1.2928... Val Loss: 1.3439\n",
      "Epoch: 13/20... Step: 1790... Loss: 1.2864... Val Loss: 1.3417\n",
      "Epoch: 13/20... Step: 1800... Loss: 1.3214... Val Loss: 1.3449\n",
      "Epoch: 14/20... Step: 1810... Loss: 1.3232... Val Loss: 1.3457\n",
      "Epoch: 14/20... Step: 1820... Loss: 1.3003... Val Loss: 1.3391\n",
      "Epoch: 14/20... Step: 1830... Loss: 1.3141... Val Loss: 1.3373\n",
      "Epoch: 14/20... Step: 1840... Loss: 1.2653... Val Loss: 1.3331\n",
      "Epoch: 14/20... Step: 1850... Loss: 1.2552... Val Loss: 1.3372\n",
      "Epoch: 14/20... Step: 1860... Loss: 1.3020... Val Loss: 1.3373\n",
      "Epoch: 14/20... Step: 1870... Loss: 1.3286... Val Loss: 1.3340\n",
      "Epoch: 14/20... Step: 1880... Loss: 1.3121... Val Loss: 1.3349\n",
      "Epoch: 14/20... Step: 1890... Loss: 1.3214... Val Loss: 1.3352\n",
      "Epoch: 14/20... Step: 1900... Loss: 1.2923... Val Loss: 1.3394\n",
      "Epoch: 14/20... Step: 1910... Loss: 1.2957... Val Loss: 1.3317\n",
      "Epoch: 14/20... Step: 1920... Loss: 1.2820... Val Loss: 1.3340\n",
      "Epoch: 14/20... Step: 1930... Loss: 1.2673... Val Loss: 1.3290\n",
      "Epoch: 14/20... Step: 1940... Loss: 1.3191... Val Loss: 1.3268\n",
      "Epoch: 15/20... Step: 1950... Loss: 1.2844... Val Loss: 1.3335\n",
      "Epoch: 15/20... Step: 1960... Loss: 1.2874... Val Loss: 1.3229\n",
      "Epoch: 15/20... Step: 1970... Loss: 1.2806... Val Loss: 1.3182\n",
      "Epoch: 15/20... Step: 1980... Loss: 1.2748... Val Loss: 1.3203\n",
      "Epoch: 15/20... Step: 1990... Loss: 1.2747... Val Loss: 1.3222\n",
      "Epoch: 15/20... Step: 2000... Loss: 1.2525... Val Loss: 1.3181\n",
      "Epoch: 15/20... Step: 2010... Loss: 1.2706... Val Loss: 1.3177\n",
      "Epoch: 15/20... Step: 2020... Loss: 1.3000... Val Loss: 1.3214\n",
      "Epoch: 15/20... Step: 2030... Loss: 1.2611... Val Loss: 1.3227\n",
      "Epoch: 15/20... Step: 2040... Loss: 1.2817... Val Loss: 1.3202\n",
      "Epoch: 15/20... Step: 2050... Loss: 1.2639... Val Loss: 1.3253\n",
      "Epoch: 15/20... Step: 2060... Loss: 1.2797... Val Loss: 1.3203\n",
      "Epoch: 15/20... Step: 2070... Loss: 1.2852... Val Loss: 1.3134\n",
      "Epoch: 15/20... Step: 2080... Loss: 1.2822... Val Loss: 1.3117\n",
      "Epoch: 16/20... Step: 2090... Loss: 1.2812... Val Loss: 1.3230\n",
      "Epoch: 16/20... Step: 2100... Loss: 1.2665... Val Loss: 1.3130\n",
      "Epoch: 16/20... Step: 2110... Loss: 1.2643... Val Loss: 1.3120\n",
      "Epoch: 16/20... Step: 2120... Loss: 1.2763... Val Loss: 1.3118\n",
      "Epoch: 16/20... Step: 2130... Loss: 1.2410... Val Loss: 1.3099\n",
      "Epoch: 16/20... Step: 2140... Loss: 1.2482... Val Loss: 1.3094\n",
      "Epoch: 16/20... Step: 2150... Loss: 1.2762... Val Loss: 1.3090\n",
      "Epoch: 16/20... Step: 2160... Loss: 1.2488... Val Loss: 1.3075\n",
      "Epoch: 16/20... Step: 2170... Loss: 1.2511... Val Loss: 1.3129\n",
      "Epoch: 16/20... Step: 2180... Loss: 1.2543... Val Loss: 1.3145\n",
      "Epoch: 16/20... Step: 2190... Loss: 1.2737... Val Loss: 1.3102\n",
      "Epoch: 16/20... Step: 2200... Loss: 1.2563... Val Loss: 1.3058\n",
      "Epoch: 16/20... Step: 2210... Loss: 1.2159... Val Loss: 1.3127\n",
      "Epoch: 16/20... Step: 2220... Loss: 1.2752... Val Loss: 1.3050\n",
      "Epoch: 17/20... Step: 2230... Loss: 1.2500... Val Loss: 1.3163\n",
      "Epoch: 17/20... Step: 2240... Loss: 1.2535... Val Loss: 1.3087\n",
      "Epoch: 17/20... Step: 2250... Loss: 1.2385... Val Loss: 1.3070\n",
      "Epoch: 17/20... Step: 2260... Loss: 1.2418... Val Loss: 1.3025\n",
      "Epoch: 17/20... Step: 2270... Loss: 1.2448... Val Loss: 1.3079\n",
      "Epoch: 17/20... Step: 2280... Loss: 1.2601... Val Loss: 1.3028\n",
      "Epoch: 17/20... Step: 2290... Loss: 1.2561... Val Loss: 1.3057\n",
      "Epoch: 17/20... Step: 2300... Loss: 1.2253... Val Loss: 1.2989\n",
      "Epoch: 17/20... Step: 2310... Loss: 1.2383... Val Loss: 1.3009\n",
      "Epoch: 17/20... Step: 2320... Loss: 1.2316... Val Loss: 1.3034\n",
      "Epoch: 17/20... Step: 2330... Loss: 1.2316... Val Loss: 1.3028\n",
      "Epoch: 17/20... Step: 2340... Loss: 1.2529... Val Loss: 1.2989\n",
      "Epoch: 17/20... Step: 2350... Loss: 1.2510... Val Loss: 1.3016\n",
      "Epoch: 17/20... Step: 2360... Loss: 1.2638... Val Loss: 1.2994\n",
      "Epoch: 18/20... Step: 2370... Loss: 1.2394... Val Loss: 1.2995\n",
      "Epoch: 18/20... Step: 2380... Loss: 1.2355... Val Loss: 1.2927\n",
      "Epoch: 18/20... Step: 2390... Loss: 1.2268... Val Loss: 1.2993\n",
      "Epoch: 18/20... Step: 2400... Loss: 1.2615... Val Loss: 1.2942\n",
      "Epoch: 18/20... Step: 2410... Loss: 1.2578... Val Loss: 1.2956\n",
      "Epoch: 18/20... Step: 2420... Loss: 1.2305... Val Loss: 1.2908\n",
      "Epoch: 18/20... Step: 2430... Loss: 1.2348... Val Loss: 1.2936\n",
      "Epoch: 18/20... Step: 2440... Loss: 1.2260... Val Loss: 1.2898\n",
      "Epoch: 18/20... Step: 2450... Loss: 1.2116... Val Loss: 1.2905\n",
      "Epoch: 18/20... Step: 2460... Loss: 1.2334... Val Loss: 1.2895\n",
      "Epoch: 18/20... Step: 2470... Loss: 1.2329... Val Loss: 1.2924\n",
      "Epoch: 18/20... Step: 2480... Loss: 1.2248... Val Loss: 1.2922\n",
      "Epoch: 18/20... Step: 2490... Loss: 1.2188... Val Loss: 1.2898\n",
      "Epoch: 18/20... Step: 2500... Loss: 1.2199... Val Loss: 1.2870\n",
      "Epoch: 19/20... Step: 2510... Loss: 1.2248... Val Loss: 1.2878\n",
      "Epoch: 19/20... Step: 2520... Loss: 1.2382... Val Loss: 1.2877\n",
      "Epoch: 19/20... Step: 2530... Loss: 1.2542... Val Loss: 1.2823\n",
      "Epoch: 19/20... Step: 2540... Loss: 1.2577... Val Loss: 1.2859\n",
      "Epoch: 19/20... Step: 2550... Loss: 1.2051... Val Loss: 1.2882\n",
      "Epoch: 19/20... Step: 2560... Loss: 1.2286... Val Loss: 1.2812\n",
      "Epoch: 19/20... Step: 2570... Loss: 1.2147... Val Loss: 1.2826\n",
      "Epoch: 19/20... Step: 2580... Loss: 1.2442... Val Loss: 1.2857\n",
      "Epoch: 19/20... Step: 2590... Loss: 1.2032... Val Loss: 1.2897\n",
      "Epoch: 19/20... Step: 2600... Loss: 1.2123... Val Loss: 1.2835\n",
      "Epoch: 19/20... Step: 2610... Loss: 1.2201... Val Loss: 1.2855\n",
      "Epoch: 19/20... Step: 2620... Loss: 1.1984... Val Loss: 1.2775\n",
      "Epoch: 19/20... Step: 2630... Loss: 1.2025... Val Loss: 1.2760\n",
      "Epoch: 19/20... Step: 2640... Loss: 1.2181... Val Loss: 1.2763\n",
      "Epoch: 20/20... Step: 2650... Loss: 1.2224... Val Loss: 1.2805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20... Step: 2660... Loss: 1.2312... Val Loss: 1.2821\n",
      "Epoch: 20/20... Step: 2670... Loss: 1.2332... Val Loss: 1.2774\n",
      "Epoch: 20/20... Step: 2680... Loss: 1.2180... Val Loss: 1.2907\n",
      "Epoch: 20/20... Step: 2690... Loss: 1.2199... Val Loss: 1.2836\n",
      "Epoch: 20/20... Step: 2700... Loss: 1.2266... Val Loss: 1.2815\n",
      "Epoch: 20/20... Step: 2710... Loss: 1.1898... Val Loss: 1.2790\n",
      "Epoch: 20/20... Step: 2720... Loss: 1.1916... Val Loss: 1.2774\n",
      "Epoch: 20/20... Step: 2730... Loss: 1.1792... Val Loss: 1.2799\n",
      "Epoch: 20/20... Step: 2740... Loss: 1.1895... Val Loss: 1.2828\n",
      "Epoch: 20/20... Step: 2750... Loss: 1.1985... Val Loss: 1.2931\n",
      "Epoch: 20/20... Step: 2760... Loss: 1.1869... Val Loss: 1.2817\n",
      "Epoch: 20/20... Step: 2770... Loss: 1.2284... Val Loss: 1.2826\n",
      "Epoch: 20/20... Step: 2780... Loss: 1.2435... Val Loss: 1.2763\n"
     ]
    }
   ],
   "source": [
    "batchSize = 128\n",
    "seqLength = 100\n",
    "nEpochs = 20 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs = nEpochs, batchSize = batchSize, seqLength = seqLength, lr = 0.001, printEvery = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_20_epoch.net'\n",
    "\n",
    "checkpoint = {'nHidden': net.nHidden,\n",
    "              'nLayers': net.nLayers,\n",
    "              'stateDict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(net, char, h = None, topK = None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.charToInt[char]]])\n",
    "        x = oneHotEncode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(trainOnGPU):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(trainOnGPU):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if topK is None:\n",
    "            topCh = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, topCh = p.topk(topK)\n",
    "            topCh = topCh.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(topCh, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.intToChar[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', topK = None):\n",
    "        \n",
    "    if(trainOnGPU):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.initHidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, topK = topK)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, topK = topK)\n",
    "        chars.append(char) \n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna Arkadyevna\n",
      "shoot her at the conversation, told her husband. \"What are the man and\n",
      "the comprehition of my watches one. You can't be all that something of\n",
      "herself instances! Yes, you know that. I am going in for things in the\n",
      "coarment too the princess, it may be as if you were in any day, in a long\n",
      "whine or this starticial time,\" said Vronsky that her\n",
      "face and the children are so looking at her stuck of the party all\n",
      "that was at the sourd, simply a famoring of the completely observantess and\n",
      "conversation with him and takons of a minute to his son, and what had sudeenly\n",
      "come to him with some son in the station of the doctor, and all on\n",
      "his head, and he felt that he was suffering and sawing and so in this standing\n",
      "to the carriage. The position when their head the carriage seemed to hild all\n",
      "such a condition, he frowned, and she had not the panifue consisting of the man she\n",
      "was aware of sick, he had a second starmond of what he was all one\n",
      "of his standinds and satisfied about it is to him f\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', topK=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_20_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], nHidden = checkpoint['nHidden'], nLayers=checkpoint['nLayers'])\n",
    "loaded.load_state_dict(checkpoint['stateDict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said:\n",
      "\"I she say all themelves, I see...\"\n",
      "\n",
      "She could not be said, where intinations were stood at home in the same\n",
      "time. The minutes senking the soft and tried to him that in the frommonest\n",
      "show, who had answered thround her.\n",
      "\n",
      "\"Well, we won't see him,\" said Levin.\n",
      "\n",
      "\"I've been a fire to ask her, and so it seemed!... There's no one,\" he said,\n",
      "like taking about the disagreeable answer, that her face atsoned\n",
      "her strictly. She had sat drew in a concistoving a chair, had been\n",
      "seefing his wife's seating open. She went up to the prayers, he was\n",
      "so intense to the door and help in the first tomeshooking and a single of severe tree at\n",
      "once and a secrative, and having had been always a crimace stood, he felt\n",
      "immoralle to the most time of her. She would have been to be something,\n",
      "and the books's wife had been to go on, and took to him all there, and went\n",
      "over a sort of treet, and he had saying her husband to the standing of his\n",
      "condition, who was still to be able to bake it, he had not all on some\n",
      "personal, and all of them as how that she were in a life, as she had been\n",
      "completely for a break of his family of sight of the same at the prayers of\n",
      "the sounds and how was in his beard. At the mander of the commission,\n",
      "was said: \"And I shall carree happy to him.\"\n",
      "\n",
      "This charmanted wearshed his chair, as though there was a servant all the\n",
      "soun as anything in his face.\n",
      "\n",
      "\"I don't look out in.... Bethes make mushriles. In the more thought, but\n",
      "I'm as to stay of it till me for the counting, which's true. It apolted the\n",
      "long while, and has to say this sought of anyone.\"\n",
      "\n",
      "\"No,\" alshe was in a soletulish, with her hostess, smiling,. \"And so the\n",
      "position of the moneich of me, I should have been so sofpering that I should be\n",
      "the face that, I'm not so that there's not to be abrecting in a carriage. And how\n",
      "don't stop?\" she thought.\n",
      "\n",
      "\"Well, we'll see. I am going,\" he said, \"then't see you.\n",
      "There are time as she discired and began. And should her father and\n",
      "the simple start as he have no other arrange, \n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, topK = 5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
